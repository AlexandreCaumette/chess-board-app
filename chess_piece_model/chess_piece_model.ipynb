{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dossier contenant les images\n",
    "data_dir = 'chemin/vers/votre/dossier'\n",
    "\n",
    "# Liste des images et des étiquettes\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Parcourir les fichiers dans le dossier\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.png'):\n",
    "        # Extraire les étiquettes du nom de fichier\n",
    "        color, piece, _ = filename.split('_')\n",
    "        label = f\"{color}_{piece}\"\n",
    "\n",
    "        # Charger l'image\n",
    "        img_path = os.path.join(data_dir, filename)\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(64, 64))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "        # Ajouter l'image et l'étiquette aux listes\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convertir les listes en tableaux numpy\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encoder les étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_encoded = tf.keras.utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer un générateur d'images pour l'augmentation des données\n",
    "datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "\n",
    "# Construire le modèle CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_val, y_val))\n",
    "\n",
    "# Évaluer le modèle\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher les images avec les prédictions et les vraies réponses\n",
    "def display_images_with_predictions_and_truth(model, images, true_labels, labels_encoder, num_images=5):\n",
    "    # Sélectionner un sous-ensemble aléatoire d'images\n",
    "    indices = np.random.choice(range(len(images)), num_images, replace=False)\n",
    "    selected_images = images[indices]\n",
    "    selected_true_labels = true_labels[indices]\n",
    "\n",
    "    # Prédire les étiquettes pour les images sélectionnées\n",
    "    predictions = model.predict(selected_images)\n",
    "    predicted_labels = labels_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    true_labels_decoded = labels_encoder.inverse_transform(np.argmax(selected_true_labels, axis=1))\n",
    "\n",
    "    # Afficher les images avec leurs prédictions et les vraies réponses\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(selected_images[i] / 255.0)  # Normaliser les valeurs des pixels pour l'affichage\n",
    "        plt.title(f\"Pred: {predicted_labels[i]}\\nTrue: {true_labels_decoded[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Assurez-vous que 'model' est votre modèle entraîné, 'X_val' est votre ensemble de validation,\n",
    "# 'y_val' est les vraies étiquettes pour l'ensemble de validation,\n",
    "# et 'label_encoder' est l'encodeur d'étiquettes utilisé pour encoder les étiquettes.\n",
    "display_images_with_predictions_and_truth(model, X_val, y_val, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\alexa\\AppData\\Local\\Temp\\tmpy8p9u8hc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\alexa\\AppData\\Local\\Temp\\tmpy8p9u8hc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\alexa\\AppData\\Local\\Temp\\tmpy8p9u8hc'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='input_layer_3')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 14), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1896638039440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638040976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638041552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638042128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638041360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638042896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638038864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638043856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638043280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896638045392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Load your trained Keras model\n",
    "model = tf.keras.models.load_model(\"chess_piece_model.keras\")\n",
    "\n",
    "# Convert to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "mobile_model_path = os.path.join(\n",
    "    '..',\n",
    "    'mobile_application',\n",
    "    'assets',\n",
    "    'chess_piece_model.tflite'\n",
    ")\n",
    "\n",
    "with open(mobile_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
